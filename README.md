# NanoGPT

NanoGPT is a compact and efficient implementation of a GPT-style language model for auto-regressive text generation. Inspired by the seminal paper "Attention Is All You Need" and further guided by architectural principles from OpenAIâ€™s GPT-2 and GPT-3 models, MiniGPT distills the core ideas into a much smaller model (~10 million parameters) that is accessible, interpretable, and fast to train.

![Transformer](https://slds-lmu.github.io/seminar_nlp_ss20/img/transformer.png)

This project focuses exclusively on the decoder-only Transformer architecture, staying faithful to the auto-regressive nature of the GPT family. It's designed for learning, experimentation, and small-scale generation tasks.

## Key Features

### Character-Level Encoding
The model operates on individual characters rather than words or subwords, enabling it to handle out-of-vocabulary tokens and learn fine-grained language patterns.

### Transformer Decoder Architecture
Implements only the decoder portion of the original Transformer model, focusing on the auto-regressive text generation task in line with the GPT series.

### Compact Size (~10 Million Parameters)
The entire model is built with approximately 10 million trainable parameters, making it lightweight enough for experimentation on consumer-grade hardware while still producing meaningful output.

### Auto-Regressive Text Generation
Capable of generating sequences of text one token at a time, predicting the next character based on previously generated context.

*The model was trained on dual NVIDIA T4 GPUs, leveraging parallel processing to efficiently handle large batches and accelerate training of the 10M parameter architecture.*

## Sample Output

Below is a sample generated by the model after training on a Shakespearean-style corpus. The model uses character-level encoding and a decoder-only Transformer architecture to generate coherent, stylistically consistent sequences:

````
KING RICHARD II:
Shal lifest made to bub, to take Our my dagatants:
Whith foul his vetward that a endrer, my fears' to zorm heavens,
Oof it heart my would but
With ensengmin latest in ov the doest not.

WARWICK:
Welll now, and thus quechiry: there's speak you love.
In Bodiet, and whom the sclittle
Enout-now what evily well most rive with is compon to the me
Town danters, If so;
Ange to shall do aleous, for dear?

KING HENRY VI:
Hark, but a
ards bring Edward?

GROKE:
As is no Rurnts I am you! who neet.
Pom mary thou contrantym so a thense.

QUEEN VINCENTIO:
O, sir, may in God't well ow, whom confessy.
Which migh.

ARCHILINIUS:
Dithul seaze Peed me: very it passce of's cruport;
How what make you fear tals: there loves
Tunkistren in deed, is xment.

CORIONIUS:
What comforts me. I with self From the walt I?

GRINION:
Which ushold.

KING HENRY Gindner:
Withrief I doot, is onter now.

Securming:
Intande whose no crown some Eiverely marry sold;
For for me watch the
our torguet! Goy, know our her and brut what I, I huself as humsell.

APTOLYCUM:
Laitance and toarth or word
As beherefitions so me worting.

CORIOLINA:
What a wouldds,
An but branedy wouldIng my a canity:
Was you be any in Becausing watcess the Regreast men is what see would in thas jury your Hrannertandless;
As there'erliacter me band frind through he crown, I she love is stay just torment:
Slaw you behoth unserving of vonby the post,
Whave baste hold; I they nengety may's fries
To there's fince, I heave arrow old,
Thee best sincess soul be
that Lord, as;
River thou a-latsteer:
Out.

PORALLINA:
Where but
Braight gentle, drieven the know you
for that to this mack a rishn. Prawity arm as is infectely,
Ah, sinstats o' no, this send; commant to love,
Go fly this fathal
I cortuns cold, offrong to old, the courtly thee? before a gace.

KING RICHARD III:
A life he pusict
It. Vitters, and were not fanturs, thy promind thy awonse than a braute comforn,
Will Roman! you brain shown'd for a dresss me; he heavison!


MENE
````


## Links and References

- [Attention Is All You Need (Paper)](https://arxiv.org/pdf/1706.03762)
- [NanoGPT (Karpathy's Repo)](https://github.com/karpathy/nanoGPT)
